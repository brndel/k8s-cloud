apiVersion: source.toolkit.fluxcd.io/v1beta2
kind: HelmRepository
metadata:
  name: prometheus-community
spec:
  interval: 1h
  url: https://prometheus-community.github.io/helm-charts
---
apiVersion: v1
kind: Secret
metadata:
  name: grafana-admin
type: Opaque
data:
  username: {{ "admin" | b64enc | quote }}
  password: {{ include "common.secrets.passwords.manage" (dict "secret" "grafana-admin" "key" "password" "providedValues" (list "") "length" 16 "context" $) }}
---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
spec:
  interval: 1h
  chart:
    spec:
      chart: kube-prometheus-stack
      version: "^46"
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
  install:
    crds: CreateReplace
  upgrade:
    crds: CreateReplace
  timeout: '15m0s' # Account for slow performance of cluster on first setup.
  values:

{{- if .Values.bootstrap }}

    kubeControllerManager:
      enabled: false
    kubeProxy:
      enabled: false
    kubeScheduler:
      enabled: false
    nodeExporter:
      enabled: false

{{- else }}

    defaultRules:
      rules:
        general: false
        {{- if and .Values.k3s (.Values.node_endpoints | len | eq 0) }}
        kubeControllerManager: false
        kubeProxy: false
        kubeSchedulerAlerting: false
        kubeSchedulerRecording: false
        {{- end }}
        {{- if .Values.vcluster }}
        nodeExporterAlerting: false
        nodeExporterRecording: false
        {{- end }}
    {{- if .Values.k3s }}
    {{- if .Values.node_endpoints | len | gt 0 }}
    kubeControllerManager:
      endpoints: {{ .Values.node_endpoints | toYaml | nindent 8 }}
    kubeProxy:
      endpoints: {{ .Values.node_endpoints | toYaml | nindent 8 }}
    kubeScheduler:
      endpoints: {{ .Values.node_endpoints | toYaml | nindent 8 }}
    {{- else }}
    kubeControllerManager:
      enabled: false
    kubeProxy:
      enabled: false
    kubeScheduler:
      enabled: false
    {{- end }}
    {{- end }}
    {{- if .Values.vcluster }}
    nodeExporter:
      enabled: false
    {{- end }}
    prometheus-node-exporter:
      resources:
        requests:
          cpu: 30m
          memory: 32Mi
    kube-state-metrics:
      resources:
        requests:
          cpu: 30m
          memory: 64Mi
    alertmanager:
      enabled: true
      alertmanagerSpec:
        resources:
          requests:
            cpu: 30m
            memory: 32Mi
    # additionalPrometheusRulesMap:
    #   deployment-errors:
    #     groups:
    #     - name: failing-k8s-resources
    #       rules:
    #       - alert: KubePodNotReady30m
    #         annotations:
    #           description: Pod {{`{{`}} $labels.namespace {{`}}`}}/{{`{{`}} $labels.pod {{`}}`}} has been in a non-ready state for longer than 30 minutes.
    #           runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepodnotready
    #           summary: Pod has been in a non-ready state for more than 30 minutes.
    #         expr: |-
    #           sum by (namespace, pod, cluster) (
    #             max by(namespace, pod, cluster) (
    #               kube_pod_status_phase{job="kube-state-metrics", namespace=~".*", phase=~"Pending|Unknown|Failed"}
    #             ) * on(namespace, pod, cluster) group_left(owner_kind) topk by(namespace, pod, cluster) (
    #               1, max by(namespace, pod, owner_kind, cluster) (kube_pod_owner{owner_kind!="Job"})
    #             )
    #           ) > 0
    #         for: 30m
    #         labels:
    #           severity: critical
    #     - name: failing-flux-resources
    #       rules:
    #       - alert: FluxReleaseNotReady30m
    #         annotations:
    #           description: Flux {{`{{`}} $labels.kind {{`}}`}} {{`{{`}} $labels.exported_namespace {{`}}`}}/{{`{{`}} $labels.name {{`}}`}} has been in a non-ready state for longer than 30 minutes.
    #           runbook_url: https://fluxcd.io/flux/cheatsheets/troubleshooting/
    #           summary: Flux release has been in a non-ready state for more than 30 minutes.
    #         expr: |-
    #           (
    #             max by(exported_namespace, name, kind) (
    #               gotk_reconcile_condition{kind=~"HelmRelease|Kustomization", type="Ready", status!="True"}
    #             ) 
    #             - 
    #             max by(exported_namespace, name, kind) (
    #               gotk_suspend_status{kind=~"HelmRelease|Kustomization"}
    #             )
    #           ) > 0
    #         for: 30m
    #         labels:
    #           severity: error
    prometheusOperator:
      admissionWebhooks:
        timeoutSeconds: 30 # Prevent timeouts during helm upgrade
        patch:
          resources:
            cpu: 30m # measured on 2023-11-30
            memory: 32Mi # measured on 2023-11-30
      resources:
        requests:
          cpu: 30m # measured on 2023-11-30
          memory: 64Mi # measured on 2023-11-30
      prometheusConfigReloader:
        resources:
          requests:
            cpu: 30m # measured on 2023-11-30
            memory: 64Mi # measured on 2023-11-30
          limits:
            cpu: 200m
            memory: 128Mi
    prometheus:
      prometheusSpec:
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelectorNilUsesHelmValues: false
        probeSelectorNilUsesHelmValues: false
        ruleSelectorNilUsesHelmValues: false
        storageSpec:
          volumeClaimTemplate:
            metadata:
              labels:
                recurring-job.longhorn.io/source: enabled
                recurring-job-group.longhorn.io/default: enabled
                recurring-job.longhorn.io/snap-delete-after-2: enabled
                velero.io/exclude-from-backup: "true"
            spec:
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 20Gi
              storageClassName: local
        retention: 3d
        resources:
          requests:
            cpu: 150m # measured on 2023-11-30
            memory: 2.5Gi # measured on 2023-11-30
          limits:
            cpu: 300m # measured on 2023-11-30
            memory: 3Gi # measured on 2023-11-30
    grafana:
      replicas: 1
      deploymentStrategy:
        type: Recreate
      resources:
        requests:
          cpu: 50m # measured on 2023-11-30
          memory: 384Mi # measured on 2023-11-30
      envValueFrom:
        SMTP_PASSWORD:
          secretKeyRef:
            name: smtp-creds
            key: password
        OIDC_SECRET:
          secretKeyRef:
            name: oidc-client
            key: secret
        LOKI_PASS:
          secretKeyRef:
            name: loki-basicauth
            key: password
      sidecar:
        alerts:
          enabled: true
      admin:
        existingSecret: grafana-admin
        userKey: username
        passwordKey: password
      grafana.ini:
        server:
          root_url: {{ printf "https://%s" .Values.host }}
        smtp:
          enabled: true
          host: {{ printf "%s:%s" .Values.smtp.host .Values.smtp.port | quote }}
          user: {{ .Values.smtp.user }}
          password: "${SMTP_PASSWORD}"
          from_address: {{ .Values.smtp.from_address }}
        auth:
          oauth_auto_login: true
          disable_login_form: true
        auth.generic_oauth:
          # See: https://grafana.com/docs/grafana/latest/setup-grafana/configure-security/configure-authentication/generic-oauth/
          enabled: true
          name: Admin OIDC
          client_id: {{ .Values.oidc_client.id }}
          client_secret: "${OIDC_SECRET}"
          scopes: openid,profile,email,groups
          auth_url: {{ printf "%s/protocol/openid-connect/auth" .Values.oidc_client.idp_url | quote }}
          token_url: {{ printf "%s/protocol/openid-connect/token" .Values.oidc_client.idp_url | quote }}
          api_url: {{ printf "%s/protocol/openid-connect/userinfo" .Values.oidc_client.idp_url | quote }}
          allow_sign_up: true
          allowed_groups: {{ .Values.admin_group | quote }}
          role_attribute_path: "contains(groups[*], {{ .Values.admin_group | squote }}) && 'Admin' || 'Viewer'"
          {{- if .Values.letsencrypt_staging }}
          tls_skip_verify_insecure: true
          {{- end }}
      ingress:
        enabled: {{ .Values.ingress }}
        annotations:
          kubernetes.io/tls-acme: "true"
        hosts:
          - {{ .Values.host }}
        tls:
          - hosts:
              - {{ .Values.host }}
            secretName: grafana-cert
      persistence:
        enabled: true
        size: 4Gi
        accessModes:
          - ReadWriteOnce
        storageClassName: ha
      alerting:
        contactpoints.yaml:
          apiVersion: 1
          contactPoints:
            - orgId: 1
              name: default
              receivers:
                - uid: default
                  type: email
                  settings:
                    addresses: {{ .Values.admin_email }}
                    singleEmail: true
        # mute-times.yaml:
        #   apiVersion: 1
        #   muteTimes:
        #     - orgId: 1
        #       name: default
        #       # refer to https://prometheus.io/docs/alerting/latest/configuration/#time_interval-0
        #       time_intervals:
        #         - times:
        #             - start_time: '00:00'
        #               end_time: '07:00'
        #     - orgId: 1
        #       name: always
        #       # refer to https://prometheus.io/docs/alerting/latest/configuration/#time_interval-0
        #       time_intervals:
        #         - times:
        #             - start_time: '00:00'
        #               end_time: '24:00'
        # policies.yaml:
        #   apiVersion: 1
        #   policies:
        #     - orgId: 1
        #       receiver: default
        #       group_by: ['...']
        #       routes:
        #         - receiver: default
        #           matchers:
        #             - severity =~ "error|critical"
        #           mute_time_intervals:
        #             - default
        #           continue: true
        #         - receiver: default
        #           mute_time_intervals:
        #             - always
        #           continue: true
      additionalDataSources:
        - name: Loki
          type: loki
          access: proxy
          url: http://loki-gateway
          basicAuth: true
          basicAuthUser: cluster
          jsonData:
            maxLines: 1000
            httpHeaderName1: "X-Scope-OrgID"
          secureJsonData:
            basicAuthPassword: "${LOKI_PASS}"
            httpHeaderValue1: "1"
---
apiVersion: v1
kind: Secret
metadata:
  name: smtp-creds
type: Opaque
data:
  {{- $passRef := .Values.smtp.password_secret }}
  password: {{ get ((lookup "v1" "Secret" (default .Release.Namespace $passRef.namespace) $passRef.name).data) $passRef.key }}
---
apiVersion: v1
kind: Secret
metadata:
  name: oidc-client
type: Opaque
data:
  {{- $secretRef := .Values.oidc_client.secret }}
  secret: {{ get ((lookup "v1" "Secret" (default .Release.Namespace $secretRef.namespace) $secretRef.name).data) $secretRef.key }}
---
{{- end }}
