- name: Set dynamic facts
  vars:
    suffix: "{{ domain | community.dns.get_public_suffix }}"
    main_name: "{{ domain | replace(suffix, '') | split('.') | last }}"
  ansible.builtin.set_fact:
    k8s_api_host: "{{ subdomains.k8s_api }}.{{ domain }}"
    charts_dir: "{{ playbook_dir }}/charts"
    cluster_name: "{{ domain | replace('.', '-') }}-cluster"
    registered_domain: "{{ main_name }}{{ suffix }}"
    host_kubeconfig: "{{ ansible_env.KUBECONFIG }}"
    kubeconfig: "{{ ansible_env.KUBECONFIG }}"
    ingress_nodes: "{{ groups['ingress_nodes'] if 'ingress_nodes' in groups else [inventory_hostname] }}"
    storage_nodes: "{{ groups['storage_nodes'] if 'storage_nodes' in groups else groups['all'] }}"

- name: Print domain info
  ansible.builtin.debug:
    msg: "Setting up cluster '{{ cluster_name }}' for domain '{{ domain }}' on registered domain '{{ registered_domain }}'"

- name: Switch to host cluster context via kubectl
  when: "host_cluster_name"
  environment:
    KUBECONFIG: "{{ host_kubeconfig }}"
  block:
    - name: Get current kubectl context via command
      ansible.builtin.command: "kubectl config current-context"
      changed_when: false
      register: default_ctx

    - name: Set kubectl context to supplied value
      ansible.builtin.command: "kubectl config use-context {{ host_cluster_name }}"
      changed_when: "host_cluster_name != default_ctx.stdout"

- name: Ensure cluster labels are applied to nodes
  environment:
    K8S_AUTH_KUBECONFIG: "{{ host_kubeconfig }}"
  block:
    - name: Get all k8s nodes
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Node
      register: all_k8s_nodes

    - name: Filter out cluster nodes
      ansible.builtin.set_fact:
        cluster_node_names: "{{ all_k8s_nodes.resources | json_query(node_filter) | json_query(node_extractor) }}"
      vars:
        node_filter: '[?contains({{ groups["all"] }}, metadata.annotations."k3s.io/external-ip")]'
        node_extractor: '[].metadata.annotations."k3s.io/hostname"'

    - name: Filter out ingress nodes
      ansible.builtin.set_fact:
        ingress_node_names: "{{ all_k8s_nodes.resources | json_query(node_filter) | json_query(node_extractor) }}"
      vars:
        node_filter: '[?contains({{ ingress_nodes }}, metadata.annotations."k3s.io/external-ip")]'
        node_extractor: '[].metadata.annotations."k3s.io/hostname"'

    - name: Filter out storage nodes
      ansible.builtin.set_fact:
        storage_node_names: "{{ all_k8s_nodes.resources | json_query(node_filter) | json_query(node_extractor) }}"
      vars:
        node_filter: '[?contains({{ storage_nodes }}, metadata.annotations."k3s.io/external-ip")]'
        node_extractor: '[].metadata.annotations."k3s.io/hostname"'

    - name: Add cluster label to inventory node
      loop: "{{ cluster_node_names }}"
      kubernetes.core.k8s:
        state: patched
        kind: Node
        name: "{{ item }}"
        definition:
          metadata:
            labels: "{{ {cluster_name: '1'} }}"

    - name: Add cluster ingress label to inventory node
      loop: "{{ ingress_node_names }}"
      kubernetes.core.k8s:
        state: patched
        kind: Node
        name: "{{ item }}"
        definition:
          metadata:
            labels:
              svccontroller.k3s.cattle.io/enablelb: "true"
              svccontroller.k3s.cattle.io/lbpool: "{{ cluster_name }}"

    - name: Add cluster storage label to inventory node
      loop: "{{ storage_node_names }}"
      vars:
        storage_key: "{{ cluster_name }}-storage"
      kubernetes.core.k8s:
        state: patched
        kind: Node
        name: "{{ item }}"
        definition:
          metadata:
            labels: "{{ {storage_key: '1'} }}"

- name: Ensure vcluster for domain exists and selected in kubeconfig
  when: "vcluster"
  environment:
    K8S_AUTH_KUBECONFIG: "{{ host_kubeconfig }}"
    KUBECONFIG: "{{ host_kubeconfig }}"
  block:
    - name: Reconcile base git repo
      ansible.builtin.command: "flux reconcile source git base-repo -n flux-system"
      changed_when: true

    - name: Ensure vcluster HelmRelease exists
      ansible.builtin.include_tasks:
        file: deploy-chart.yaml
        apply:
          vars:
            chart:
              name: vcluster-stack
              namespace: "{{ cluster_name }}"
              chart_values:
                cluster_name: "{{ cluster_name }}"
                host: "{{ k8s_api_host }}"
                oidc_client:
                  idp_url: "https://{{ subdomains.keycloak }}.{{ domain }}/realms/master"
                  id: cluster-oidc
            kubeconfig: "{{ host_kubeconfig }}"

    - name: Wait for vcluster deployment to become ready
      kubernetes.core.k8s_info:
        kind: Deployment
        namespace: "{{ cluster_name }}"
        name: vcluster
        wait: true
        wait_timeout: 600

    - name: Gather facts on listening ports
      community.general.listen_ports_facts:
        include_non_listening: true

    - name: Select port to forward vcluster api over
      vars:
        used_ports: "{{ ansible_facts.tcp_listen | map(attribute='port') | list }}"
      ansible.builtin.set_fact:
        vcluster_port: "{{ range(5000, 30000) | list | difference(used_ports) | random }}"

    - name: Generate kubeconfig for vcluster
      vars:
        connect_flags:
          - "--namespace={{ cluster_name }}"
          - "--local-port={{ vcluster_port }}"
          - "--update-current=false"
          - "--kube-config=/tmp/virtual-kubeconfig.yaml"
          - "--kube-config-context-name={{ cluster_name }}"
      ansible.builtin.shell: "nohup vcluster connect vcluster {{ connect_flags | join(' ') }} </dev/null >/dev/null 2>&1 &"
      changed_when: true

    - name: Print port where vcluster listens
      ansible.builtin.debug:
        msg: "VCluster API is being forwarded to port {{ vcluster_port }} on localhost"

    - name: Wait for port-forwarded vcluster API to become available
      ansible.builtin.wait_for:
        host: localhost
        port: "{{ vcluster_port }}"
        timeout: 120

    - name: Activate vcluster kubeconfig
      ansible.builtin.set_fact:
        kubeconfig: /tmp/virtual-kubeconfig.yaml

    - name: Wait for vcluster coredns deployment to become available
      environment:
        K8S_AUTH_KUBECONFIG: "{{ kubeconfig }}"
      kubernetes.core.k8s_info:
        kind: Deployment
        namespace: kube-system
        name: coredns
        wait: true
        wait_timeout: 120

- name: Ensure kubeconfig file is available on local machine
  block:
    - name: Determine kubeconfig context name
      vars:
        host_ctx: "{{ host_cluster_name | ternary(host_cluster_name, default_ctx) }}"
      ansible.builtin.set_fact:
        ctx_name: "{{ vcluster | ternary(cluster_name, host_ctx) }}"

    - name: Use remote kubeconfig file as basis
      block:
        - name: Copy remote kubeconfig file
          ansible.builtin.copy:
            remote_src: true
            src: "{{ kubeconfig }}"
            dest: /tmp/public-kubeconfig.yaml
            mode: '0644'

        - name: List remote kubeconfig contexts
          environment:
            KUBECONFIG: /tmp/public-kubeconfig.yaml
          ansible.builtin.command: "kubectl config get-contexts -o name"
          register: all_ctx
          changed_when: false

        - name: Delete all other remote contexts
          loop: "{{ all_ctx.stdout_lines }}"
          when: "item != ctx_name"
          environment:
            KUBECONFIG: /tmp/public-kubeconfig.yaml
          ansible.builtin.command: "kubectl config delete-context {{ item }}"
          changed_when: true

        - name: Delete all other remote clusters
          loop: "{{ all_ctx.stdout_lines }}"
          when: "item != ctx_name"
          environment:
            KUBECONFIG: /tmp/public-kubeconfig.yaml
          ansible.builtin.command: "kubectl config delete-cluster {{ item }}"
          changed_when: true

        - name: Delete all other remote users
          loop: "{{ all_ctx.stdout_lines }}"
          when: "item != ctx_name"
          environment:
            KUBECONFIG: /tmp/public-kubeconfig.yaml
          ansible.builtin.command: "kubectl config delete-user {{ item }}"
          changed_when: true

    - name: Check for local kubeconfig
      delegate_to: localhost
      become: false
      ansible.builtin.stat:
        path: "{{ playbook_dir }}/{{ local_kubeconfig }}"
      register: local_kubeconfig_file

    - name: Copy and merge local config file
      when: "local_kubeconfig_file.stat.exists"
      block:
        - name: Copy local kubeconfig to remote
          ansible.builtin.copy:
            src: "{{ playbook_dir }}/{{ local_kubeconfig }}"
            dest: /tmp/local-kubeconfig.yaml
            mode: '0644'

        - name: Delete existing context in local config
          environment:
            KUBECONFIG: /tmp/local-kubeconfig.yaml
          ansible.builtin.command: "kubectl config delete-context {{ ctx_name }}"
          changed_when: true
          failed_when: false

        - name: Delete existing cluster in local config
          environment:
            KUBECONFIG: /tmp/local-kubeconfig.yaml
          ansible.builtin.command: "kubectl config delete-cluster {{ ctx_name }}"
          changed_when: true
          failed_when: false

        - name: Delete existing user in local config
          environment:
            KUBECONFIG: /tmp/local-kubeconfig.yaml
          ansible.builtin.command: "kubectl config delete-user {{ ctx_name }}"
          changed_when: true
          failed_when: false

        - name: Merge local kubeconfig with remote
          environment:
            KUBECONFIG: "/tmp/public-kubeconfig.yaml:/tmp/local-kubeconfig.yaml"
          ansible.builtin.command:
            argv:
              - kubectl
              - config
              - view
              - --flatten
          register: merged_kubeconfig
          changed_when: false

        - name: Save merged kubeconfig to file
          ansible.builtin.copy:
            content: "{{ merged_kubeconfig.stdout }}"
            dest: /tmp/public-kubeconfig.yaml
            mode: '0644'

    - name: Change server address of copied kubeconfig to public cluster FQN
      environment:
        KUBECONFIG: /tmp/public-kubeconfig.yaml
      ansible.builtin.command:
        argv:
          - kubectl
          - config
          - set-cluster
          - "{{ ctx_name }}"
          - "--server=https://{{ k8s_api_host }}"
      changed_when: true

    - name: Download kubeconfig file
      ansible.builtin.fetch:
        src: /tmp/public-kubeconfig.yaml
        dest: "{{ playbook_dir }}/kubeconfig.yaml"
        flat: true

- name: Ensure GitOps via FluxCD is setup
  environment:
    K8S_AUTH_KUBECONFIG: "{{ kubeconfig }}"
    KUBECONFIG: "{{ kubeconfig }}"
  block:
    - name: Check if FluxCD release already exists
      kubernetes.core.helm_info:
        name: fluxcd
        release_namespace: flux-system
      register: existing_fluxcd

    - name: Deploy FluxCD
      when: "existing_fluxcd.status is undefined or existing_fluxcd.status.keys() | length == 0"
      kubernetes.core.helm:
        wait: true
        timeout: 10m
        name: fluxcd
        release_namespace: flux-system
        create_namespace: true
        chart_repo_url: https://fluxcd-community.github.io/helm-charts
        chart_ref: flux2
        chart_version: "^2"
        values:
          policies:
            create: false

    - name: Ensure base GitRepository exists
      kubernetes.core.k8s:
        wait: true
        definition:
          apiVersion: source.toolkit.fluxcd.io/v1
          kind: GitRepository
          metadata:
            name: base-repo
            namespace: flux-system
          spec:
            interval: 1h
            url: "{{ base_repo }}"
            ref:
              branch: "{{ base_branch }}"

    - name: Reconcile base git repo
      ansible.builtin.command: "flux reconcile source git base-repo -n flux-system"
      changed_when: true

- name: Ensure components with CRDs and other hard dependecies are bootstrapped
  ansible.builtin.include_tasks:
    file: bootstrap-chart.yaml
    apply:
      vars:
        chart: "{{ item }}"
        kubeconfig: "{{ kubeconfig }}"
  loop:
    - name: storage-stack
      namespace: longhorn-system
    - name: cert-stack
      namespace: cert-system
    - name: telemetry-stack
      namespace: telemetry-system

- name: Ensure cluster FQN label is applied to Longhorn nodes
  environment:
    K8S_AUTH_KUBECONFIG: "{{ host_kubeconfig }}"
  block:
    - name: Get list of current Longhorn node tags
      loop: "{{ storage_node_names }}"
      kubernetes.core.k8s_info:
        api_version: longhorn.io/v1beta2
        kind: Node
        namespace: longhorn-system
        name: "{{ item }}"
        wait: true
        wait_timeout: 180
        wait_condition:
          type: Ready
          status: "True"
      register: lhn_info

    - name: Ensure cluster FQN is tag on all Longhorn nodes in inventory
      loop: "{{ lhn_info.results }}"
      loop_control:
        label: "{{ item.item }}"
      vars:
        existing_tags: "{{ item.resources[0].spec.tags }}"
        lhn_name: "{{ item.resources[0].metadata.name }}"
      kubernetes.core.k8s:
        state: patched
        api_version: longhorn.io/v1beta2
        kind: Node
        namespace: longhorn-system
        name: "{{ lhn_name }}"
        definition:
          spec:
            tags: "{{ (existing_tags + [cluster_name]) | unique | list }}"
      register: lhn_patch_result
      retries: 6 # Longhorn CRD webhook tends to be unavailable at times.
      delay: 20
      until: "lhn_patch_result is not failed"

- name: Ensure all required secrets exist
  environment:
    K8S_AUTH_KUBECONFIG: "{{ kubeconfig }}"
  kubernetes.core.k8s:
    wait: true
    definition:
      apiVersion: v1
      kind: Secret
      metadata:
        name: "{{ item.name }}"
        namespace: kube-system
      type: Opaque
      data:
        secret: "{{ vars[item.inventoryKey] | b64encode }}"
  loop:
    - inventoryKey: smtp_password
      name: smtp
    - inventoryKey: s3_access_key_secret
      name: s3
    - inventoryKey: external_dns_token
      name: external-dns

- name: Ensure pre-restore infrastructure is fully set up
  ansible.builtin.include_tasks:
    file: deploy-chart.yaml
    apply:
      vars:
        chart: "{{ item }}"
        kubeconfig: "{{ kubeconfig }}"
  loop_control:
    label: "{{ item.name }} in {{ item.namespace }}"
  loop:
    - name: storage-stack
      namespace: longhorn-system
      chart_values:
        host: "{{ subdomains.longhorn }}.{{ domain }}"
        cluster_name: "{{ cluster_name }}"
        oauth2_proxy_host: "{{ subdomains.oauth2_proxy }}.{{ domain }}"
        admin_group: "cluster-admins"
        s3: "{{ backups.s3 | combine({'access_key_secret': {'name': 's3', 'namespace': 'kube-system', 'key': 'secret'}}) }}"

    - name: backup-stack
      namespace: backup-system
      chart_values:
        s3: "{{ backups.s3 | combine({'access_key_secret': {'name': 's3', 'namespace': 'kube-system', 'key': 'secret'}}) }}"

- name: Restore cluster from backup
  when: "restore.enabled"
  vars:
    restore_name: "{{ restore.backup_name | ternary(restore.backup_name, restore.schedule_name) }}"
  block:
    - name: Wait for backup resource to be synced from s3
      when: "restore.backup_name"
      kubernetes.core.k8s_info:
        api_version: velero.io/v1
        kind: Backup
        namespace: backup-system
        name: "{{ restore.backup_name }}"
      register: backup_get_result
      retries: 10 # Velero may need time to load the backup from s3.
      delay: 10
      until: "backup_get_result.resources | length > 0"

    - name: Check if restoration for given backup already exists
      kubernetes.core.k8s_info:
        api_version: velero.io/v1
        kind: Restore
        namespace: backup-system
        name: "{{ restore_name }}"
      register: existing_restore

    - name: Apply backup restoration
      when: "existing_restore.resources | length == 0"
      environment:
        KUBECONFIG: "{{ kubeconfig }}"
      ansible.builtin.command:
        argv:
          - velero
          - restore
          - create
          - "{{ restore_name }}"
          - "--namespace=backup-system"
          - "--from-schedule={{ restore.backup_name | ternary('', restore.schedule_name) }}"
          - "--from-backup={{ restore.backup_name }}"
          - "--include-resources={{ restore.included_resources | join(',') }}"
          - "--restore-volumes=true"
          - "--selector"
          - "owner notin (helm)"
          - "--wait"
      changed_when: true

- name: Ensure remaining infrastructure is fully set up
  ansible.builtin.include_tasks:
    file: deploy-chart.yaml
    apply:
      vars:
        chart: "{{ item }}"
        kubeconfig: "{{ kubeconfig }}"
  loop_control:
    label: "{{ item.name }} in {{ item.namespace }}"
  loop:
    - name: admin-rbac
      namespace: kube-system
      chart_values:
        admin_group: "cluster-admins"

    - name: k3s-auto-upgrade
      namespace: upgrade-system

    - name: cert-stack
      namespace: cert-system
      chart_values:
        admin_email: "{{ mail.admin_address }}"
        base_repo: "{{ base_repo }}"
        base_branch: "{{ base_branch }}"
        letsencrypt_staging: "{{ letsencrypt_staging }}"

    - name: ingress-stack
      namespace: ingress-system
      chart_values:
        cluster_name: "{{ cluster_name }}"

    - name: control-stack
      namespace: kube-system
      chart_values:
        host: "{{ subdomains.k8s_dashboard }}.{{ domain }}"
        oauth2_proxy_host: "{{ subdomains.oauth2_proxy }}.{{ domain }}"
        admin_group: "cluster-admins"

    - name: dns-stack
      namespace: dns-system
      chart_values:
        domain: "{{ registered_domain }}"
        token_secret:
          namespace: kube-system
          name: external-dns
          key: secret

    - name: sso-stack
      namespace: sso-system
      chart_values:
        domain: "{{ domain }}"
        org: "{{ org }}"
        hosts:
          keycloak: "{{ subdomains.keycloak }}.{{ domain }}"
          authproxy: "{{ subdomains.oauth2_proxy }}.{{ domain }}"
        cluster_client:
          id: cluster-oidc
          redirect_uris:
            - "https://{{ subdomains.grafana }}.{{ domain }}/login/generic_oauth"
            - "https://{{ subdomains.weave_gitops }}.{{ domain }}/oauth2/callback"
        smtp: "{{ smtp | combine({'password_secret': {'name': 'smtp', 'namespace': 'kube-system', 'key': 'secret'}}) }}"
        admin_group: "cluster-admins"
        admin_email: "{{ mail.admin_address }}"
        letsencrypt_staging: "{{ letsencrypt_staging }}"

    - name: telemetry-stack
      namespace: telemetry-system
      chart_values:
        domain: "{{ domain }}"
        org: "{{ org }}"
        host: "{{ subdomains.grafana }}.{{ domain }}"
        oidc_client:
          idp_url: "https://{{ subdomains.keycloak }}.{{ domain }}/realms/master"
          id: "cluster-oidc"
          secret:
            namespace: sso-system
            name: cluster-oidc
            key: secret
        admin_email: "{{ mail.admin_address }}"
        admin_group: "cluster-admins"
        smtp: "{{ smtp | combine({'password_secret': {'name': 'smtp', 'namespace': 'kube-system', 'key': 'secret'}}) }}"
        node_endpoints: "{{ groups['all'] }}"
        letsencrypt_staging: "{{ letsencrypt_staging }}"
        k3s: true

    - name: cicd-stack
      namespace: flux-system
      chart_values:
        host: "{{ subdomains.weave_gitops }}.{{ domain }}"
        oidc_client:
          idp_url: "https://{{ subdomains.keycloak }}.{{ domain }}/realms/master"
          id: "cluster-oidc"
          secret:
            namespace: sso-system
            name: cluster-oidc
            key: secret
        letsencrypt_staging: "{{ letsencrypt_staging }}"

- name: Expose k8s API via Ingress
  environment:
    K8S_AUTH_KUBECONFIG: "{{ kubeconfig }}"
  kubernetes.core.k8s:
    wait: true
    definition:
      apiVersion: networking.k8s.io/v1
      kind: Ingress
      metadata:
        name: k8s-api
        namespace: default
        annotations:
          nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
          nginx.ingress.kubernetes.io/ssl-passthrough: "true"
          nginx.ingress.kubernetes.io/ssl-redirect: "true"
      spec:
        rules:
          - host: "{{ k8s_api_host }}"
            http:
              paths:
                - backend:
                    service:
                      name: kubernetes
                      port:
                        number: 443
                  path: /
                  pathType: ImplementationSpecific
  register: k8s_ingress_result
  retries: 6 # NGINX webhook tends to be unavailable at times.
  delay: 20
  until: "k8s_ingress_result is not failed"

- name: Reset kubectl context
  when: "host_cluster_name and default_ctx is defined"
  environment:
    KUBECONFIG: "{{ host_kubeconfig }}"
  ansible.builtin.command: "kubectl config use-context {{ default_ctx.stdout }}"
  changed_when: "host_cluster_name != default_ctx.stdout"

- name: Cleanup port forwarding
  when: "vcluster"
  block:
    - name: Get PIDs of running vcluster port-forwarders
      community.general.pids:
        name: vcluster
      register: vcluster_pids

    - name: Terminate extraneous port-forwarders to vcluster
      when: "vcluster_pids.pids | length > 1"
      loop: "{{ vcluster_pids.pids[:-1] }}"
      ansible.builtin.command: "kill -SIGTERM {{ item }}"
      changed_when: true
