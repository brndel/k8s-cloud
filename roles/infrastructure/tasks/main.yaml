- name: Set dynamic facts
  vars:
    suffix: "{{ domain | community.dns.get_public_suffix }}"
    main_name: "{{ domain | replace(suffix, '') | split('.') | last }}"
  ansible.builtin.set_fact:
    cluster_fqn: "{{ subdomains.cluster }}.{{ domain }}"
    charts_dir: "{{ playbook_dir }}/charts"
    cluster_name: "{{ domain | replace('.', '-') }}"
    registered_domain: "{{ main_name }}{{ suffix }}"
    host_kubeconfig: "{{ ansible_env.KUBECONFIG }}"
    kubeconfig: "{{ ansible_env.KUBECONFIG }}"

- name: Print domain info
  ansible.builtin.debug:
    msg: "Setting up cluster '{{ cluster_fqn }}' for domain '{{ domain }}' on registered domain '{{ registered_domain }}'"

- name: Switch to host cluster context via kubectl
  when: "host_cluster_name"
  environment:
    KUBECONFIG: "{{ host_kubeconfig }}"
  block:
    - name: Get current kubectl context via command
      ansible.builtin.command: "kubectl config current-context"
      changed_when: false
      register: default_ctx

    - name: Set kubectl context to supplied value
      ansible.builtin.command: "kubectl config use-context {{ host_cluster_name }}"
      changed_when: "host_cluster_name != default_ctx.stdout"

- name: Ensure cluster FQN label is applied to nodes
  environment:
    K8S_AUTH_KUBECONFIG: "{{ host_kubeconfig }}"
  block:
    - name: Get all k8s nodes
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Node
      register: all_k8s_nodes

    - name: Filter out nodes as per inventory
      ansible.builtin.set_fact:
        inventory_nodes: "{{ all_k8s_nodes.resources | json_query(node_filter) | json_query(node_extractor) }}"
      vars:
        node_filter: '[?contains({{ groups["all"] }}, metadata.annotations."k3s.io/external-ip")]'
        node_extractor: '[].metadata.annotations."k3s.io/hostname"'

    - name: Add cluster FQN as label to all inventory nodes
      loop: "{{ inventory_nodes }}"
      kubernetes.core.k8s:
        state: patched
        kind: Node
        name: "{{ item }}"
        definition:
          metadata:
            labels: "{{ {cluster_fqn: '1'} }}"

- name: Ensure vcluster for domain exists
  when: "vcluster"
  environment:
    K8S_AUTH_KUBECONFIG: "{{ host_kubeconfig }}"
    KUBECONFIG: "{{ host_kubeconfig }}"
  block:
    - name: Ensure vcluster helm release exists
      kubernetes.core.helm:
        wait: true
        timeout: "10m0s"
        name: vcluster
        release_namespace: "{{ cluster_name }}"
        create_namespace: true
        chart_repo_url: https://charts.loft.sh
        chart_ref: vcluster
        chart_version: "'^0.15.3 || 0.15.3-beta.0'" # Need this fix: https://github.com/loft-sh/vcluster/pull/1089
        values:
          sync:
            nodes:
              enabled: true
              nodeSelector: "{{ cluster_fqn }}=1"
            persistentvolumes:
              enabled: true
            storageclasses:
              enabled: true
            volumesnapshots:
              enabled: true
            ingresses:
              enabled: true
            networkpolicies:
              enabled: true
          proxy:
            metricsServer:
              nodes:
                enabled: true
              pods:
                enabled: true
          hostpathMapper:
            enabled: true
          syncer:
            extraArgs:
              - "--tls-san={{ cluster_fqn }}"
          vcluster:
            extraArgs:
              - "--kube-apiserver-arg=oidc-issuer-url=https://{{ subdomains.keycloak }}.{{ domain }}/realms/master"
              - "--kube-apiserver-arg=oidc-client-id=cluster-oidc"
              - "--kube-apiserver-arg=oidc-groups-claim=groups"
          plugin:
            cert-manager-plugin:
              image: ghcr.io/loft-sh/vcluster-plugins/cert-manager-plugin:0.3.0
              imagePullPolicy: IfNotPresent
              rbac:
                role:
                  extraRules:
                    - apiGroups: ["cert-manager.io"]
                      resources: ["issuers", "certificates"]
                      verbs: ["create", "delete", "patch", "update", "get", "list", "watch"]
                clusterRole:
                  extraRules:
                    - apiGroups: ["apiextensions.k8s.io"]
                      resources: ["customresourcedefinitions"]
                      verbs: ["get", "list", "watch"]

    - name: Gather facts on listening ports
      community.general.listen_ports_facts:
        include_non_listening: true

    - name: Switch kube-context to vcluster
      vars:
        used_ports: "{{ ansible_facts.tcp_listen | map(attribute='port') | list }}"
      ansible.builtin.set_fact:
        vcluster_port: "{{ range(5000, 30000) | list | difference(used_ports) | random }}"

    - name: Generate kubeconfig for vcluster
      vars:
        connect_flags:
          - "--namespace={{ cluster_name }}"
          - "--local-port={{ vcluster_port }}"
          - "--update-current=false"
          - "--kube-config=/tmp/virtual-kubeconfig.yaml"
          - "--kube-config-context-name={{ cluster_name }}"
      ansible.builtin.shell: "nohup vcluster connect vcluster {{ connect_flags | join(' ') }} </dev/null >/dev/null 2>&1 &"
      changed_when: true

    - name: Print port where vcluster listens
      ansible.builtin.debug:
        msg: "VCluster API is being forwarded to port {{ vcluster_port }} on localhost"

    - name: Wait for port-forwarded vcluster API to become available
      ansible.builtin.wait_for:
        host: localhost
        port: "{{ vcluster_port }}"
        timeout: 120

    - name: Activate vcluster kubeconfig
      ansible.builtin.set_fact:
        kubeconfig: /tmp/virtual-kubeconfig.yaml

- name: Ensure kubeconfig file is available on local machine
  block:
    - name: Determine kubeconfig context name
      vars:
        host_ctx: "{{ host_cluster_name | ternary(host_cluster_name, default_ctx) }}"
      ansible.builtin.set_fact:
        ctx_name: "{{ vcluster | ternary(cluster_name, host_ctx) }}"

    - name: Use remote kubeconfig file as basis
      block:
        - name: Copy remote kubeconfig file
          ansible.builtin.copy:
            remote_src: true
            src: "{{ kubeconfig }}"
            dest: /tmp/public-kubeconfig.yaml
            mode: '0644'

        - name: List remote kubeconfig contexts
          environment:
            KUBECONFIG: /tmp/public-kubeconfig.yaml
          ansible.builtin.command: "kubectl config get-contexts -o name"
          register: all_ctx
          changed_when: false

        - name: Delete all other remote contexts
          loop: "{{ all_ctx.stdout_lines }}"
          when: "item != ctx_name"
          environment:
            KUBECONFIG: /tmp/public-kubeconfig.yaml
          ansible.builtin.command: "kubectl config delete-context {{ item }}"
          changed_when: true

        - name: Delete all other remote clusters
          loop: "{{ all_ctx.stdout_lines }}"
          when: "item != ctx_name"
          environment:
            KUBECONFIG: /tmp/public-kubeconfig.yaml
          ansible.builtin.command: "kubectl config delete-cluster {{ item }}"
          changed_when: true

        - name: Delete all other remote users
          loop: "{{ all_ctx.stdout_lines }}"
          when: "item != ctx_name"
          environment:
            KUBECONFIG: /tmp/public-kubeconfig.yaml
          ansible.builtin.command: "kubectl config delete-user {{ item }}"
          changed_when: true

    - name: Check for local kubeconfig
      delegate_to: localhost
      become: false
      ansible.builtin.stat:
        path: "{{ playbook_dir }}/{{ local_kubeconfig }}"
      register: local_kubeconfig_file

    - name: Copy and merge local config file
      when: "local_kubeconfig_file.stat.exists"
      block:
        - name: Copy local kubeconfig to remote
          ansible.builtin.copy:
            src: "{{ playbook_dir }}/{{ local_kubeconfig }}"
            dest: /tmp/local-kubeconfig.yaml
            mode: '0644'

        - name: Delete existing context in local config
          environment:
            KUBECONFIG: /tmp/local-kubeconfig.yaml
          ansible.builtin.command: "kubectl config delete-context {{ ctx_name }}"
          changed_when: true
          failed_when: false

        - name: Delete existing cluster in local config
          environment:
            KUBECONFIG: /tmp/local-kubeconfig.yaml
          ansible.builtin.command: "kubectl config delete-cluster {{ ctx_name }}"
          changed_when: true
          failed_when: false

        - name: Delete existing user in local config
          environment:
            KUBECONFIG: /tmp/local-kubeconfig.yaml
          ansible.builtin.command: "kubectl config delete-user {{ ctx_name }}"
          changed_when: true
          failed_when: false

        - name: Merge local kubeconfig with remote
          environment:
            KUBECONFIG: "/tmp/public-kubeconfig.yaml:/tmp/local-kubeconfig.yaml"
          ansible.builtin.command:
            argv:
              - kubectl
              - config
              - view
              - --flatten
          register: merged_kubeconfig
          changed_when: false

        - name: Save merged kubeconfig to file
          ansible.builtin.copy:
            content: "{{ merged_kubeconfig.stdout }}"
            dest: /tmp/public-kubeconfig.yaml
            mode: '0644'

    - name: Change server address of copied kubeconfig to public cluster FQN
      environment:
        KUBECONFIG: /tmp/public-kubeconfig.yaml
      ansible.builtin.command:
        argv:
          - kubectl
          - config
          - set-cluster
          - "{{ ctx_name }}"
          - "--server=https://{{ cluster_fqn }}"
      changed_when: true

    - name: Download kubeconfig file
      ansible.builtin.fetch:
        src: /tmp/public-kubeconfig.yaml
        dest: "{{ playbook_dir }}/kubeconfig.yaml"
        flat: true

- name: Ensure GitOps is setup
  environment:
    K8S_AUTH_KUBECONFIG: "{{ kubeconfig }}"
    KUBECONFIG: "{{ kubeconfig }}"
  block:
    - name: Check if FluxCD release already exists
      kubernetes.core.helm_info:
        name: fluxcd
        release_namespace: flux-system
      register: existing_fluxcd

    - name: Deploy FluxCD
      when: "existing_fluxcd.status is undefined or existing_fluxcd.status.keys() | length == 0"
      kubernetes.core.helm:
        wait: true
        timeout: 10m
        name: fluxcd
        release_namespace: flux-system
        create_namespace: true
        chart_repo_url: https://fluxcd-community.github.io/helm-charts
        chart_ref: flux2
        chart_version: "^2"
        values:
          policies:
            create: false

    - name: Ensure base GitRepository exists
      kubernetes.core.k8s:
        wait: true
        definition:
          apiVersion: source.toolkit.fluxcd.io/v1
          kind: GitRepository
          metadata:
            name: base-repo
            namespace: flux-system
          spec:
            interval: 1h
            url: "{{ base_repo }}"
            ref:
              branch: "{{ base_branch }}"

    - name: Reconcile base git repo
      ansible.builtin.command: "flux reconcile source git base-repo -n flux-system"
      changed_when: true

- name: Bootstrap components with CRDs and other hard dependecies (via minimal-config release)
  block:
    - name: Bootstrap host-cluster infrastructure
      when: "not vcluster"
      ansible.builtin.include_tasks:
        file: bootstrap-chart.yaml
        apply:
          vars:
            chart: "{{ item }}"
            kubeconfig: "{{ host_kubeconfig }}"
      loop:
        - name: storage-stack
          namespace: longhorn-system
        - name: cert-stack
          namespace: cert-system
        - name: ingress-stack
          namespace: ingress-system

    - name: Bootstrap cluster infrastructure
      ansible.builtin.include_tasks:
        file: bootstrap-chart.yaml
        apply:
          vars:
            chart: "{{ item }}"
            kubeconfig: "{{ kubeconfig }}"
      loop:
        - name: telemetry-stack
          namespace: telemetry-system
        - name: cicd-stack
          namespace: flux-system

- name: Ensure all required secrets exist
  environment:
    K8S_AUTH_KUBECONFIG: "{{ kubeconfig }}"
  kubernetes.core.k8s:
    wait: true
    definition:
      apiVersion: v1
      kind: Secret
      metadata:
        name: "{{ item.name }}"
        namespace: kube-system
      type: Opaque
      data:
        secret: "{{ vars[item.inventoryKey] | b64encode }}"
  loop:
    - inventoryKey: smtp_password
      name: smtp
    - inventoryKey: s3_access_key_secret
      name: s3
    - inventoryKey: external_dns_token
      name: external-dns

- name: Ensure HelmReleases for host-cluster infrastructure exist
  when: "not vcluster"
  ansible.builtin.include_tasks:
    file: deploy-chart.yaml
    apply:
      vars:
        chart: "{{ item }}"
        kubeconfig: "{{ host_kubeconfig }}"
  loop_control:
    label: "{{ item.name }} in {{ item.namespace }}"
  loop:
    - name: upgrade-controller
      namespace: kube-system

    # - name: namespace-isolation
    #   namespace: longhorn-system

    - name: storage-stack
      namespace: longhorn-system
      chart_values:
        host: "{{ subdomains.longhorn }}.{{ domain }}"
        oauth2_proxy_host: "{{ subdomains.oauth2_proxy }}.{{ domain }}"
        admin_group: "cluster-admins"
        s3: "{{ s3 | combine({'access_key_secret': {'name': 's3', 'namespace': 'kube-system', 'key': 'secret'}, 'bucket': s3_buckets.backup}) }}"

    # # Can't isolate cert-system with current policy approach.
    # # This is due to k8s api server trying to call cert-manager webhook and timing out.
    # - name: namespace-isolation
    #   namespace: cert-system

    - name: cert-stack
      namespace: cert-system
      chart_values:
        admin_email: "{{ admin_email }}"
        base_repo: "{{ base_repo }}"
        base_branch: "{{ base_branch }}"
        letsencrypt_staging: "{{ letsencrypt_staging }}"

    # - name: namespace-isolation
    #   namespace: ingress-system

    - name: ingress-stack
      namespace: ingress-system

    # - name: namespace-isolation
    #   namespace: backup-system

    - name: backup-stack
      namespace: backup-system
      chart_values:
        s3: "{{ s3 | combine({'access_key_secret': {'name': 's3', 'namespace': 'kube-system', 'key': 'secret'}, 'bucket': s3_buckets.backup}) }}"

- name: Expose k8s API via Ingress
  environment:
    K8S_AUTH_KUBECONFIG: "{{ host_kubeconfig }}"
  kubernetes.core.k8s:
    wait: true
    definition:
      apiVersion: networking.k8s.io/v1
      kind: Ingress
      metadata:
        name: k8s-api
        namespace: "{{ vcluster | ternary(cluster_name, 'default') }}"
        annotations:
          nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
          nginx.ingress.kubernetes.io/ssl-passthrough: "true"
          nginx.ingress.kubernetes.io/ssl-redirect: "true"
      spec:
        rules:
          - host: "{{ cluster_fqn }}"
            http:
              paths:
                - backend:
                    service:
                      name: "{{ vcluster | ternary('vcluster', 'kubernetes') }}"
                      port:
                        number: 443
                  path: /
                  pathType: ImplementationSpecific
  register: k8s_ingress_result
  retries: 6 # NGINX webhook tends to be unavailable at times.
  delay: 20
  until: "k8s_ingress_result is not failed"

- name: Ensure cluster FQN label is applied to Longhorn nodes
  environment:
    K8S_AUTH_KUBECONFIG: "{{ host_kubeconfig }}"
  block:
    - name: Get list of current Longhorn node tags
      loop: "{{ inventory_nodes }}"
      kubernetes.core.k8s_info:
        api_version: longhorn.io/v1beta2
        kind: Node
        namespace: longhorn-system
        name: "{{ item }}"
        wait: true
        wait_timeout: 180
        wait_condition:
          type: Ready
          status: "True"
      register: lhn_info

    - name: Ensure cluster FQN is tag on all Longhorn nodes in inventory
      loop: "{{ lhn_info.results }}"
      loop_control:
        label: "{{ item.item }}"
      vars:
        existing_tags: "{{ item.resources[0].spec.tags }}"
        lhn_name: "{{ item.resources[0].metadata.name }}"
      kubernetes.core.k8s:
        state: patched
        api_version: longhorn.io/v1beta2
        kind: Node
        namespace: longhorn-system
        name: "{{ lhn_name }}"
        definition:
          spec:
            tags: "{{ (existing_tags + [cluster_fqn]) | unique | list }}"
      register: lhn_patch_result
      retries: 6 # Longhorn CRD webhook tends to be unavailable at times.
      delay: 20
      until: "lhn_patch_result is not failed"

- name: Apply backup restoration, if supplied
  when: "not vcluster and restore_from_backup"
  environment:
    K8S_AUTH_KUBECONFIG: "{{ host_kubeconfig }}"
  kubernetes.core.k8s:
    wait: true
    definition:
      apiVersion: velero.io/v1
      kind: Restore
      metadata:
        name: initial-cluster-restore
        namespace: backup-system
      spec:
        backupName: "{{ restore_from_backup }}"

- name: Ensure HelmReleases for cluster infrastructure exist
  ansible.builtin.include_tasks:
    file: deploy-chart.yaml
    apply:
      vars:
        chart: "{{ item }}"
        kubeconfig: "{{ kubeconfig }}"
  loop_control:
    label: "{{ item.name }} in {{ item.namespace }}"
  loop:
    - name: admin-rbac
      namespace: kube-system
      chart_values:
        admin_group: "cluster-admins"

    - name: cluster-storage
      namespace: kube-system
      chart_values:
        cluster_fqn: "{{ cluster_fqn }}"
        vcluster: "{{ vcluster }}"

    - name: control-stack
      namespace: kube-system
      chart_values:
        host: "{{ subdomains.dashboard }}.{{ domain }}"
        oauth2_proxy_host: "{{ subdomains.oauth2_proxy }}.{{ domain }}"
        admin_group: "cluster-admins"

    # - name: namespace-isolation
    #   namespace: dns-system

    - name: dns-stack
      namespace: dns-system
      chart_values:
        domain: "{{ registered_domain }}"
        token_secret:
          namespace: kube-system
          name: external-dns
          key: secret

    - name: sso-stack
      namespace: sso-system
      chart_values:
        domain: "{{ domain }}"
        org: "{{ org }}"
        hosts:
          keycloak: "{{ subdomains.keycloak }}.{{ domain }}"
          authproxy: "{{ subdomains.oauth2_proxy }}.{{ domain }}"
        cluster_client:
          id: cluster-oidc
          redirect_uris:
            - "https://{{ subdomains.grafana }}.{{ domain }}/login/generic_oauth"
            - "https://{{ subdomains.weave_gitops }}.{{ domain }}/oauth2/callback"
        smtp: "{{ smtp | combine({'password_secret': {'name': 'smtp', 'namespace': 'kube-system', 'key': 'secret'}}) }}"
        admin_group: "cluster-admins"
        admin_email: "{{ admin_email }}"
        letsencrypt_staging: "{{ letsencrypt_staging }}"

    - name: telemetry-stack
      namespace: telemetry-system
      chart_values:
        domain: "{{ domain }}"
        org: "{{ org }}"
        host: "{{ subdomains.grafana }}.{{ domain }}"
        oidc_client:
          idp_url: "https://{{ subdomains.keycloak }}.{{ domain }}/realms/master"
          id: "cluster-oidc"
          secret:
            namespace: sso-system
            name: cluster-oidc
            key: secret
        admin_email: "{{ admin_email }}"
        admin_group: "cluster-admins"
        s3: "{{ s3 | combine({'access_key_secret': {'name': 's3', 'namespace': 'kube-system', 'key': 'secret'}, 'bucket': s3_buckets.logs}) }}"
        smtp: "{{ smtp | combine({'password_secret': {'name': 'smtp', 'namespace': 'kube-system', 'key': 'secret'}}) }}"
        node_endpoints: "{{ groups['all'] }}"
        letsencrypt_staging: "{{ letsencrypt_staging }}"
        k3s: true
        vcluster: "{{ vcluster }}"

    # # Produces 502 bad gateway when trying to access weave-gitops via ingress
    # - name: namespace-isolation
    #   namespace: flux-system

    - name: cicd-stack
      namespace: flux-system
      chart_values:
        host: "{{ subdomains.weave_gitops }}.{{ domain }}"
        oidc_client:
          idp_url: "https://{{ subdomains.keycloak }}.{{ domain }}/realms/master"
          id: "cluster-oidc"
          secret:
            namespace: sso-system
            name: cluster-oidc
            key: secret
        letsencrypt_staging: "{{ letsencrypt_staging }}"

- name: Reset kubectl context
  when: "host_cluster_name and default_ctx is defined"
  environment:
    KUBECONFIG: "{{ host_kubeconfig }}"
  ansible.builtin.command: "kubectl config use-context {{ default_ctx.stdout }}"
  changed_when: "host_cluster_name != default_ctx.stdout"

- name: Cleanup port forwarding
  when: "vcluster"
  block:
    - name: Get PIDs of running vcluster port-forwarders
      community.general.pids:
        name: vcluster
      register: vcluster_pids

    - name: Terminate extraneous port-forwarders to vcluster
      when: "vcluster_pids.pids | length > 1"
      loop: "{{ vcluster_pids.pids[:-1] }}"
      ansible.builtin.command: "kill -SIGTERM {{ item }}"
      changed_when: true
