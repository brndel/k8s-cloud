- name: Load configuration
  ansible.builtin.import_role:
    name: cluster-config
    allow_duplicates: false

- name: Set dynamic facts
  vars:
    suffix: "{{ cluster.domain | community.dns.get_public_suffix }}"
    main_name: "{{ cluster.domain | replace(suffix, '') | split('.') | last }}"
  ansible.builtin.set_fact:
    k8s_api_host: "{{ cluster.subdomains.k8s_api }}.{{ cluster.domain }}"
    charts_dir: "{{ playbook_dir }}/charts"
    cluster_name: "{{ cluster.domain | replace('.', '-') }}-cluster"
    registered_domain: "{{ main_name }}{{ suffix }}"
    kubeconfig: "{{ kubeconfig if kubeconfig else ansible_env.KUBECONFIG if 'KUBECONFIG' in ansible_env else '/etc/rancher/k3s/k3s.yaml' }}"

- name: Set host kubeconfig
  ansible.builtin.set_fact:
    host_kubeconfig: "{{ kubeconfig }}"

- name: Print cluster.domain info
  ansible.builtin.debug:
    msg: "Setting up cluster '{{ cluster_name }}' for cluster.domain '{{ cluster.domain }}' on registered cluster.domain '{{ registered_domain }}'"

- name: Switch to host cluster context via kubectl
  when: "host_cluster_name"
  environment:
    KUBECONFIG: "{{ host_kubeconfig }}"
  block:
    - name: Get current kubectl context via command
      ansible.builtin.command: "kubectl config current-context"
      changed_when: false
      register: default_ctx

    - name: Set kubectl context to supplied value
      ansible.builtin.command: "kubectl config use-context {{ host_cluster_name }}"
      changed_when: "host_cluster_name != default_ctx.stdout"

- name: Wait for Kubernetes API to become available
  environment:
    KUBECONFIG: "{{ host_kubeconfig }}"
  ansible.builtin.command: kubectl cluster-info
  register: kubectl_info_result
  retries: 6
  delay: 20
  until: "kubectl_info_result is not failed"
  changed_when: false

- name: Ensure cluster labels are applied to nodes
  environment:
    K8S_AUTH_KUBECONFIG: "{{ host_kubeconfig }}"
  block:
    - name: Add cluster label to inventory node
      loop: "{{ groups['cluster'] }}"
      kubernetes.core.k8s:
        state: patched
        kind: Node
        name: "{{ item }}"
        definition:
          metadata:
            labels: "{{ {cluster_name: '1'} }}"

    - name: Add topology labels to inventory node
      loop: "{{ groups['cluster'] }}"
      vars:
        region: "{{ hostvars[item].region | default('eu') }}"
        zone: "{{ hostvars[item].zone | default('eu-1') }}"
      kubernetes.core.k8s:
        state: patched
        kind: Node
        name: "{{ item }}"
        definition:
          metadata:
            labels:
              topology.kubernetes.io/region: "{{ region }}"
              topology.kubernetes.io/zone: "{{ zone }}"

    - name: Add backbone label to inventory node
      loop: "{{ groups['backbone'] }}"
      kubernetes.core.k8s:
        state: patched
        kind: Node
        name: "{{ item }}"
        definition:
          metadata:
            labels:
              backbone: "1"

    - name: Add cluster ingress label to inventory node
      loop: "{{ groups['ingress'] }}"
      kubernetes.core.k8s:
        state: patched
        kind: Node
        name: "{{ item }}"
        definition:
          metadata:
            labels:
              svccontroller.k3s.cattle.io/enablelb: "true"
              svccontroller.k3s.cattle.io/lbpool: "{{ cluster_name }}"

    - name: Add cluster storage label to inventory node
      loop: "{{ groups['storage'] }}"
      vars:
        storage_key: "{{ cluster_name }}-storage"
      kubernetes.core.k8s:
        state: patched
        kind: Node
        name: "{{ item }}"
        definition:
          metadata:
            labels: "{{ {storage_key: '1'} }}"

- name: Ensure vcluster for cluster.domain exists and selected in kubeconfig
  when: "cluster.virtual"
  environment:
    K8S_AUTH_KUBECONFIG: "{{ host_kubeconfig }}"
    KUBECONFIG: "{{ host_kubeconfig }}"
  block:
    - name: Reconcile base git repo
      ansible.builtin.command: "flux reconcile source git base-repo -n flux-system"
      changed_when: true

    - name: Ensure vcluster HelmRelease exists
      ansible.builtin.include_tasks:
        file: deploy-chart.yaml
        apply:
          vars:
            chart:
              name: vcluster-stack
              namespace: "{{ cluster_name }}"
              chart_values:
                cluster_name: "{{ cluster_name }}"
                host: "{{ k8s_api_host }}"
                oidc_client:
                  idp_url: "https://{{ cluster.subdomains.keycloak }}.{{ cluster.domain }}/realms/master"
                max_replicas: "{{ groups['cluster'] | length }}"
            kubeconfig: "{{ host_kubeconfig }}"

    - name: Wait for vcluster deployment to become ready
      kubernetes.core.k8s_info:
        kind: Deployment
        namespace: "{{ cluster_name }}"
        name: vcluster
        wait: true
        wait_timeout: 600

    - name: Gather facts on listening ports
      community.general.listen_ports_facts:
        include_non_listening: true

    - name: Select port to forward vcluster api over
      vars:
        used_ports: "{{ ansible_facts.tcp_listen | map(attribute='port') | list }}"
      ansible.builtin.set_fact:
        vcluster_port: "{{ range(5000, 30000) | list | difference(used_ports) | random }}"

    - name: Generate kubeconfig for vcluster
      vars:
        connect_flags:
          - "--namespace={{ cluster_name }}"
          - "--local-port={{ vcluster_port }}"
          - "--update-current=false"
          - "--kube-config=/tmp/virtual-kubeconfig.yaml"
          - "--kube-config-context-name={{ cluster_name }}"
      ansible.builtin.shell: "nohup vcluster connect vcluster {{ connect_flags | join(' ') }} </dev/null >/dev/null 2>&1 &"
      changed_when: true

    - name: Print port where vcluster listens
      ansible.builtin.debug:
        msg: "VCluster API is being forwarded to port {{ vcluster_port }} on localhost"

    - name: Wait for port-forwarded vcluster API to become available
      ansible.builtin.wait_for:
        host: localhost
        port: "{{ vcluster_port }}"
        timeout: 120

    - name: Activate vcluster kubeconfig
      ansible.builtin.set_fact:
        kubeconfig: /tmp/virtual-kubeconfig.yaml

    - name: Wait for vcluster coredns deployment to become available
      environment:
        K8S_AUTH_KUBECONFIG: "{{ kubeconfig }}"
      kubernetes.core.k8s_info:
        kind: Deployment
        namespace: kube-system
        name: coredns
        wait: true
        wait_timeout: 120

- name: Ensure GitOps via FluxCD is setup
  environment:
    K8S_AUTH_KUBECONFIG: "{{ kubeconfig }}"
    KUBECONFIG: "{{ kubeconfig }}"
  block:
    - name: Check if FluxCD release already exists
      kubernetes.core.helm_info:
        name: fluxcd
        release_namespace: flux-system
      register: existing_fluxcd

    - name: Deploy FluxCD
      when: "existing_fluxcd.status is undefined or existing_fluxcd.status.keys() | length == 0"
      kubernetes.core.helm:
        wait: true
        timeout: 10m
        name: fluxcd
        release_namespace: flux-system
        create_namespace: true
        chart_repo_url: https://fluxcd-community.github.io/helm-charts
        chart_ref: flux2
        chart_version: "^2"
        values:
          policies:
            create: false

    - name: Ensure base GitRepository exists
      kubernetes.core.k8s:
        wait: true
        definition:
          apiVersion: source.toolkit.fluxcd.io/v1
          kind: GitRepository
          metadata:
            name: base-repo
            namespace: flux-system
          spec:
            interval: 1h
            url: "{{ cluster.base_repo }}"
            ref:
              branch: "{{ cluster.base_branch }}"

    - name: Reconcile base git repo
      ansible.builtin.command: "flux reconcile source git base-repo -n flux-system"
      changed_when: true

- name: Ensure components with CRDs and other hard dependecies are bootstrapped
  ansible.builtin.include_tasks:
    file: bootstrap-chart.yaml
    apply:
      vars:
        chart: "{{ item }}"
        kubeconfig: "{{ kubeconfig }}"
  loop:
    - name: storage-stack
      namespace: longhorn-system
    - name: cert-stack
      namespace: cert-system
    - name: telemetry-stack
      namespace: telemetry-system

- name: Ensure zoning tags are applied to Longhorn nodes
  environment:
    K8S_AUTH_KUBECONFIG: "{{ host_kubeconfig }}"
  block:
    - name: Get list of current Longhorn node tags
      loop: "{{ groups['storage'] }}"
      kubernetes.core.k8s_info:
        api_version: longhorn.io/v1beta2
        kind: Node
        namespace: longhorn-system
        name: "{{ item }}"
        wait: true
        wait_timeout: 180
        wait_condition:
          type: Ready
          status: "True"
      register: lhn_info

    - name: Ensure tags are set on all Longhorn nodes in inventory
      loop: "{{ lhn_info.results }}"
      loop_control:
        label: "{{ item.item }}"
      vars:
        existing_tags: "{{ item.resources[0].spec.tags }}"
        node: "{{ item.resources[0].metadata.name }}"
        region: "{{ hostvars[node].region | default('eu') }}"
        zone: "{{ hostvars[node].zone | default('eu-1') }}"
        cluster_tag: "{{ cluster_name }}"
        region_tag: "{{ ('%s.%s' | format(region, cluster_tag)) }}"
        zone_tag: "{{ ('%s.%s' | format(zone, cluster_tag)) }}"
        backbone_tag: "{{ ('backbone.%s' | format(cluster_tag)) if node in groups['backbone'] else false }}"
        # Assuming here that regions and zones do not share names.
        backbone_region_tag: "{{ ('backbone.%s' | format(region_tag)) if backbone_tag else false }}"
        backbone_zone_tag: "{{ ('backbone.%s' | format(zone_tag)) if backbone_tag else false }}"
        new_tags: "{{ [cluster_tag, backbone_tag, region_tag, zone_tag, backbone_region_tag, backbone_zone_tag] | select() }}"
      kubernetes.core.k8s:
        state: patched
        api_version: longhorn.io/v1beta2
        kind: Node
        namespace: longhorn-system
        name: "{{ node }}"
        definition:
          spec:
            tags: "{{ (existing_tags + new_tags) | unique | list }}"
      register: lhn_patch_result
      retries: 6 # Longhorn CRD webhook tends to be unavailable at times.
      delay: 20
      until: "lhn_patch_result is not failed"

- name: Store cluster config
  environment:
    K8S_AUTH_KUBECONFIG: "{{ kubeconfig }}"
  kubernetes.core.k8s:
    wait: true
    definition:
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: cluster
        namespace: kube-system
      data:
        cluster.yaml: "{{ cluster | to_yaml }}"

- name: Store cluster secrets
  environment:
    K8S_AUTH_KUBECONFIG: "{{ kubeconfig }}"
  kubernetes.core.k8s:
    wait: true
    definition:
      apiVersion: v1
      kind: Secret
      metadata:
        name: "{{ item.key }}"
        namespace: kube-system
      type: Opaque
      stringData: "{{ item.value }}"
  loop: "{{ cluster_secrets | dict2items }}"
  loop_control:
    label: "{{ item.key }}"

- name: Ensure pre-restore infrastructure is fully set up
  ansible.builtin.include_tasks:
    file: deploy-chart.yaml
    apply:
      vars:
        chart: "{{ item }}"
        kubeconfig: "{{ kubeconfig }}"
  loop_control:
    label: "{{ item.name }} in {{ item.namespace }}"
  loop:
    - name: storage-stack
      namespace: longhorn-system
      chart_values:
        host: "{{ cluster.subdomains.longhorn }}.{{ cluster.domain }}"
        cluster_name: "{{ cluster_name }}"
        oauth2_proxy_host: "{{ cluster.subdomains.oauth2_proxy }}.{{ cluster.domain }}"
        admin_group: "cluster-admins"
        s3: "{{ cluster.backup.s3 }}"

    - name: backup-stack
      namespace: backup-system
      chart_values:
        s3: "{{ cluster.backup.s3 }}"

- name: Restore cluster from backup
  when: "cluster.backup.restore.enabled"
  vars:
    restore_name: "{{ cluster.backup.restore.backup_name | ternary(backup.restore.backup_name, cluster.backup.restore.schedule_name) }}"
  block:
    - name: Wait for backup resource to be synced from s3
      when: "backup.restore.backup_name"
      kubernetes.core.k8s_info:
        api_version: velero.io/v1
        kind: Backup
        namespace: backup-system
        name: "{{ cluster.backup.restore.backup_name }}"
      register: backup_get_result
      retries: 10 # Velero may need time to load the backup from s3.
      delay: 20
      until: "backup_get_result.resources | length > 0"

    - name: Check if restoration for given backup already exists
      kubernetes.core.k8s_info:
        api_version: velero.io/v1
        kind: Restore
        namespace: backup-system
        name: "{{ restore_name }}"
      register: existing_restore

    - name: Apply backup restoration
      when: "existing_restore.resources | length == 0"
      environment:
        KUBECONFIG: "{{ kubeconfig }}"
      ansible.builtin.command:
        argv:
          - velero
          - restore
          - create
          - "{{ restore_name }}"
          - "--namespace=backup-system"
          - "--from-schedule={{ cluster.backup.restore.backup_name | ternary('', cluster.backup.restore.schedule_name) }}"
          - "--from-backup={{ cluster.backup.restore.backup_name }}"
          - "--include-resources={{ cluster.backup.restore.included_resources | join(',') }}"
          - "--restore-volumes=true"
          - "--selector"
          - "owner notin (helm)"
          - "--wait"
      changed_when: true

- name: Ensure dns-stack is deployed
  when: "cluster.dns.enabled"
  ansible.builtin.include_tasks:
    file: deploy-chart.yaml
    apply:
      vars:
        chart:
          name: dns-stack
          namespace: dns-system
          chart_values:
            domain: "{{ registered_domain }}"
            provider:
              name: "{{ cluster.dns.provider }}"
              extra_config: "{{ cluster.dns.extra_config }}"
        kubeconfig: "{{ kubeconfig }}"

- name: Ensure remaining infrastructure is fully set up
  ansible.builtin.include_tasks:
    file: deploy-chart.yaml
    apply:
      vars:
        chart: "{{ item }}"
        kubeconfig: "{{ kubeconfig }}"
  loop_control:
    label: "{{ item.name }} in {{ item.namespace }}"
  loop:
    - name: admin-rbac
      namespace: kube-system
      chart_values:
        admin_group: "cluster-admins"

    - name: k3s-auto-upgrade
      namespace: upgrade-system

    - name: cert-stack
      namespace: cert-system
      chart_values:
        admin_email: "{{ cluster.mail.admin_address }}"
        base_repo: "{{ cluster.base_repo }}"
        base_branch: "{{ cluster.base_branch }}"
        letsencrypt_staging: "{{ cluster.letsencrypt_staging }}"

    - name: ingress-stack
      namespace: ingress-system
      chart_values:
        cluster_name: "{{ cluster_name }}"

    - name: control-stack
      namespace: kube-system
      chart_values:
        host: "{{ cluster.subdomains.k8s_dashboard }}.{{ cluster.domain }}"
        oauth2_proxy_host: "{{ cluster.subdomains.oauth2_proxy }}.{{ cluster.domain }}"
        admin_group: "cluster-admins"

    - name: sso-stack
      namespace: sso-system
      chart_values:
        domain: "{{ cluster.domain }}"
        org: "{{ cluster.org }}"
        hosts:
          keycloak: "{{ cluster.subdomains.keycloak }}.{{ cluster.domain }}"
          authproxy: "{{ cluster.subdomains.oauth2_proxy }}.{{ cluster.domain }}"
        cluster_client:
          id: cluster-oidc
          redirect_uris:
            - "https://{{ cluster.subdomains.grafana }}.{{ cluster.domain }}/login/generic_oauth"
            - "https://{{ cluster.subdomains.weave_gitops }}.{{ cluster.domain }}/oauth2/callback"
        smtp: "{{ cluster.mail.smtp }}"
        admin_group: "cluster-admins"
        admin_email: "{{ cluster.mail.admin_address }}"
        letsencrypt_staging: "{{ cluster.letsencrypt_staging }}"

    - name: telemetry-stack
      namespace: telemetry-system
      chart_values:
        domain: "{{ cluster.domain }}"
        org: "{{ cluster.org }}"
        host: "{{ cluster.subdomains.grafana }}.{{ cluster.domain }}"
        oidc_client:
          idp_url: "https://{{ cluster.subdomains.keycloak }}.{{ cluster.domain }}/realms/master"
        admin_email: "{{ cluster.mail.admin_address }}"
        admin_group: "cluster-admins"
        smtp: "{{ cluster.mail.smtp }}"
        node_endpoints: "{{ groups['all'] }}"
        letsencrypt_staging: "{{ cluster.letsencrypt_staging }}"
        k3s: true

    - name: cicd-stack
      namespace: flux-system
      chart_values:
        host: "{{ cluster.subdomains.weave_gitops }}.{{ cluster.domain }}"
        oidc_client:
          idp_url: "https://{{ cluster.subdomains.keycloak }}.{{ cluster.domain }}/realms/master"
        letsencrypt_staging: "{{ cluster.letsencrypt_staging }}"

- name: Expose k8s API via Ingress
  environment:
    K8S_AUTH_KUBECONFIG: "{{ kubeconfig }}"
  kubernetes.core.k8s:
    wait: true
    definition:
      apiVersion: networking.k8s.io/v1
      kind: Ingress
      metadata:
        name: k8s-api
        namespace: default
        annotations:
          nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
          nginx.ingress.kubernetes.io/ssl-redirect: "true"
          kubernetes.io/tls-acme: "true"
      spec:
        tls:
          - hosts:
              - "{{ k8s_api_host }}"
            secretName: k8s-api-cert
        rules:
          - host: "{{ k8s_api_host }}"
            http:
              paths:
                - backend:
                    service:
                      name: kubernetes
                      port:
                        number: 443
                  path: /
                  pathType: ImplementationSpecific

- name: Ensure OIDC kubeconfig file is available on local machine
  block:
    - name: Get cluster-oidc secret
      environment:
        KUBECONFIG: "{{ kubeconfig }}"
      kubernetes.core.k8s_info:
        kind: Secret
        namespace: sso-system
        name: cluster-oidc
      register: cluster_oidc_secret
      retries: 10 # Velero may need time to load the backup from s3.
      delay: 20
      until: "cluster_oidc_secret.resources | length > 0"

    - name: Template out public kubeconfig
      vars:
        api_url: "https://{{ k8s_api_host }}"
        issuer_url: "https://{{ cluster.subdomains.keycloak }}.{{ cluster.domain }}/realms/master"
        client_secret: "{{ cluster_oidc_secret.resources[0].data.secret | b64decode }}"
      ansible.builtin.template:
        src: public-kubeconfig.yaml
        dest: /tmp/public-kubeconfig.yaml
        mode: '0644'

    - name: Check for local kubeconfig
      delegate_to: localhost
      become: false
      ansible.builtin.stat:
        path: "{{ playbook_dir }}/{{ local_kubeconfig }}"
      register: local_kubeconfig_file

    - name: Copy and merge local config file
      when: "local_kubeconfig_file.stat.exists"
      block:
        - name: Copy local kubeconfig to remote
          ansible.builtin.copy:
            src: "{{ playbook_dir }}/{{ local_kubeconfig }}"
            dest: /tmp/local-kubeconfig.yaml
            mode: '0644'

        - name: Delete existing context in local config
          environment:
            KUBECONFIG: /tmp/local-kubeconfig.yaml
          ansible.builtin.command: "kubectl config delete-context {{ cluster_name }}"
          changed_when: true
          failed_when: false

        - name: Delete existing cluster in local config
          environment:
            KUBECONFIG: /tmp/local-kubeconfig.yaml
          ansible.builtin.command: "kubectl config delete-cluster {{ cluster_name }}"
          changed_when: true
          failed_when: false

        - name: Delete existing user in local config
          environment:
            KUBECONFIG: /tmp/local-kubeconfig.yaml
          ansible.builtin.command: "kubectl config delete-user {{ cluster_name }}"
          changed_when: true
          failed_when: false

        - name: Merge local kubeconfig with remote
          environment:
            KUBECONFIG: "/tmp/public-kubeconfig.yaml:/tmp/local-kubeconfig.yaml"
          ansible.builtin.command:
            argv:
              - kubectl
              - config
              - view
              - --flatten
          register: merged_kubeconfig
          changed_when: false

        - name: Save merged kubeconfig to file
          ansible.builtin.copy:
            content: "{{ merged_kubeconfig.stdout }}"
            dest: /tmp/public-kubeconfig.yaml
            mode: '0644'

    - name: Download kubeconfig file
      ansible.builtin.fetch:
        src: /tmp/public-kubeconfig.yaml
        dest: "{{ playbook_dir }}/kubeconfig.yaml"
        flat: true

- name: Ensure idp admin password is available on local machine
  block:
    - name: Get idp-admin secret
      environment:
        KUBECONFIG: "{{ kubeconfig }}"
      kubernetes.core.k8s_info:
        kind: Secret
        namespace: sso-system
        name: idp-admin
      register: idp_admin_secret
      retries: 10 # Velero may need time to load the backup from s3.
      delay: 20
      until: "idp_admin_secret.resources | length > 0"

    - name: Save idp admin password to file
      ansible.builtin.copy:
        content: "{{ idp_admin_secret.resources[0].data['admin-password'] | b64decode }}"
        dest: /tmp/admin-password
        mode: '0644'

    - name: Download idp admin password
      ansible.builtin.fetch:
        src: /tmp/admin-password
        dest: "{{ playbook_dir }}/admin-password"
        flat: true

- name: Reset kubectl context
  when: "host_cluster_name and default_ctx is defined"
  environment:
    KUBECONFIG: "{{ host_kubeconfig }}"
  ansible.builtin.command: "kubectl config use-context {{ default_ctx.stdout }}"
  changed_when: "host_cluster_name != default_ctx.stdout"

- name: Cleanup port forwarding
  when: "cluster.virtual"
  block:
    - name: Get PIDs of running vcluster port-forwarders
      community.general.pids:
        name: vcluster
      register: vcluster_pids

    - name: Terminate extraneous port-forwarders to vcluster
      when: "vcluster_pids.pids | length > 1"
      loop: "{{ vcluster_pids.pids[:-1] }}"
      ansible.builtin.command: "kill -SIGTERM {{ item }}"
      changed_when: true
